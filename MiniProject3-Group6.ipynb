{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# ! pip install gensim\n",
    "# ! pip install numpy scipy\n",
    "# ! pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/manishrawat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to load the data from the provided folder into your Python environment. We can use the os library to navigate through the folder and load the text files. Once we have loaded the data, we can build a corpus by splitting the input data into individual articles and appending them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the folder containing the data\n",
    "folder_path = 'Articles'\n",
    "\n",
    "# Load the text files into a list\n",
    "articles = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        articles.append(text)\n",
    "        \n",
    "# Create a corpus object containing all the articles\n",
    "corpus = ' '.join(articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a corpus, we will need to clean it up by separating meta-data from the actual articles. \n",
    "\n",
    "We can do this by using regular expressions to match and remove any headers or tags that appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define regular expressions to match meta-data\n",
    "header_pattern = re.compile(r'(?m)^[A-Z ]+:')\n",
    "tag_pattern = re.compile(r'\\[[A-Z ]+\\]')\n",
    "\n",
    "# Remove meta-data from the corpus\n",
    "corpus = header_pattern.sub('', corpus)\n",
    "corpus = tag_pattern.sub('', corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to extract features from the corpus. This involves tokenizing the text and removing any stop words or words that appear too frequently or infrequently. \n",
    "\n",
    "We can use the **nltk library** to tokenize the text and filter out stop words, and the **gensim** library to remove words that appear too frequently or infrequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if not word.lower() in stop_words]\n",
    "\n",
    "# Remove words that appear too frequently or infrequently\n",
    "frequency = nltk.FreqDist(filtered_tokens)\n",
    "frequency_filter = {word: count for word, count in frequency.items() if count > 10 and count < 1000}\n",
    "dictionary = corpora.Dictionary([frequency_filter.keys()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create some summaries of the features to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus: 2202621\n"
     ]
    }
   ],
   "source": [
    "# Corpus size\n",
    "\n",
    "print(\"Number of words in corpus:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 62645\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size by counting unique words in the corpus\n",
    "\n",
    "print(\"Vocabulary size:\", len(set(filtered_tokens)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create a word cloud of the most frequent words in the corpus using the wordcloud library, or could create a histogram of the word frequencies using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a string of all the filtered tokens\n",
    "text = ' '.join(filtered_tokens)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color='white', max_words=100).generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(15, 8), facecolor=None)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
