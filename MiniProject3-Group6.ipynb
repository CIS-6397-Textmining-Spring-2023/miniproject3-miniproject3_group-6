{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# ! pip install gensim\n",
    "# ! pip install numpy scipy\n",
    "# ! pip install pyLDAvis\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and exploratory data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to load the data from the provided folder into your Python environment. We can use the os library to navigate through the folder and load the text files. Once we have loaded the data, we can build a corpus by splitting the input data into individual articles and appending them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the Articles folder\n",
    "data_folder = 'Articles'\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "articles = []\n",
    "for file in files:\n",
    "    with open(os.path.join(data_folder, file), 'r', encoding='utf-8') as f:\n",
    "        articles.append(f.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "clean up the corpus, make sure you separate meta-data from the actual articles, tokenize, remove stop words, lemmatize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up corpus and separate meta-data from articles\n",
    "article_text = []\n",
    "for article in articles:\n",
    "    sections = article.split('============')\n",
    "    if len(sections) > 2:\n",
    "        article_text.append(''.join(sections[2:]))\n",
    "    else:\n",
    "        article_text.append(''.join(sections))\n",
    "\n",
    "# Tokenize, remove stop words, and lemmatize the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and words less than 3 characters long\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words and len(word) > 2]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_text = [preprocess_text(article) for article in article_text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create a word cloud of the most frequent words in the corpus using the wordcloud library, or could create a histogram of the word frequencies using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in corpus using Word Cloud\n",
    "\n",
    "word_counts = {}\n",
    "for article in processed_text:\n",
    "    for word in article:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count'])\n",
    "word_counts_df = word_counts_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Create word clouds\n",
    "wordcloud = WordCloud(width=1000, height=500,background_color=\"white\", contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate_from_frequencies(word_counts)\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words in the corpus data\n",
    "\n",
    "top10 = word_counts_df['count'][:10]\n",
    "print(\"The Top 10 words in the corpus are:\")\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distribution plot of top 20 features\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=word_counts_df['count'][:20], y=word_counts_df.index[:20], color='steelblue')\n",
    "plt.title('Top 20 most common words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require necessary libraries like gensim for topic modeling, and matplotlib and pyLDAvis for visualization.\n",
    "\n",
    "The documents are preprocessed by tokenizing them into words and removing stop words. \n",
    "\n",
    "The preprocessed documents are stored in the \"texts\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents by tokenizing and removing stop words\n",
    "\n",
    "texts = [[word for word in simple_preprocess(doc) if word not in stop_words] for doc in articles]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dictionary and bag-of-words representation of the corpus are created using the \"Dictionary\" and \"doc2bow\" functions from the \"corpora\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and bag-of-words representation of the corpus\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to creat a function to calculate coherence_score which is defined to train LDA models with different numbers of topics and calculate coherence scores for each model. \n",
    "\n",
    "The coherence score measures the quality of the topics generated by a model.\n",
    "\n",
    "The function takes in the bag-of-words corpus, dictionary, preprocessed documents, and a range of numbers of topics to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Topic Modelling \n",
    "\n",
    "def calculate_coherence_score(corpus, dictionary, texts, limit=13, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        print(f\"Number of topics: {num_topics}, Coherence Score: {coherence_score}\")\n",
    "        # write summary to output file\n",
    "        output_file_path = os.path.join(f'summary_{num_topics}_topics.txt')\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(f'Summary for {num_topics} topics:\\n\\n')\n",
    "            for topic in model.show_topics():\n",
    "                f.write(f'Topic {topic[0]}: {topic[1]}\\n\\n')\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the topic models with different number of topics & calculate the coherence score and store summaries into output files\n",
    "\n",
    "model_list, coherence_values = calculate_coherence_score(bow_corpus, dictionary, texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coherence score for different number of topics\n",
    "\n",
    "x = range(2, 13, 1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on the coherence score\n",
    "\n",
    "best_model_index = coherence_values.index(max(coherence_values))\n",
    "best_model = model_list[best_model_index]\n",
    "print(f\"Best Model - Number of Topics: {best_model.num_topics}, Coherence Score: {max(coherence_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics and their corresponding words for the best model\n",
    "\n",
    "for topic in best_model.show_topics(num_topics=best_model.num_topics, num_words=10, formatted=False):\n",
    "    print(f\"\\n Topic {topic[0]+1}: {[word[0] for word in topic[1]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualizing Topic Models with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics using pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "vis = gensimvis.prepare(best_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of topic modeling is a useful tool in analyzing how terrorist organizations are portrayed in traditional media outlets. The study found that the most common words in the corpus suggest a focus on American politics and foreign policy, with articles related to the Islamic State and its activities. The study also identified that the corpus has 10 dominant topics related to the Islamic State, terrorism, and foreign policy. Future studies could utilize similar methods to gain a deeper understanding of how terrorist organizations are portrayed in traditional media outlets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
